{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers.core import Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('https://github.com/ozt-ca/tjo.hatenablog.samples/raw/master/r_samples/public_lib/jp/exp_uci_datasets/online_news_popularity/ONP_train.csv')\n",
    "df_test = pd.read_csv('https://github.com/ozt-ca/tjo.hatenablog.samples/raw/master/r_samples/public_lib/jp/exp_uci_datasets/online_news_popularity/ONP_test.csv')\n",
    "\n",
    "x_train = df_train.iloc[:, :58]\n",
    "y_train = df_train[\"shares\"]\n",
    "x_test = df_test.iloc[:, :58]\n",
    "y_test = df_test[\"shares\"]\n",
    "\n",
    "merged_train = pd.concat([x_train, x_test])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "merged_train.iloc[:, 0:10] = scaler.fit_transform(merged_train.iloc[:, 0:10])\n",
    "merged_train.iloc[:, 17:28] = scaler.fit_transform(merged_train.iloc[:, 17:28])\n",
    "merged_train.iloc[:, 37:] = scaler.fit_transform(merged_train.iloc[:, 37:])\n",
    "\n",
    "x_train = merged_train.iloc[:len(df_train), :]\n",
    "x_test = merged_train.iloc[len(df_train):, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = shuffle(df_train)\n",
    "idx = int(np.round(len(df_train) * 0.9, 0))\n",
    "x_ptrain = df_train.iloc[:idx, :58]\n",
    "y_ptrain = df_train.iloc[:idx, 58]\n",
    "x_ptest = df_train.iloc[idx:, :58]\n",
    "y_ptest = df_train.iloc[idx:, 58]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInitialization\u001b[0m\n",
      "\u001b[94m-------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |        lr |   num_epoch |     unit1 |     unit2 |     unit3 | \n",
      "    1 | 01m02s | \u001b[35m  -0.91264\u001b[0m | \u001b[32m   0.0082\u001b[0m | \u001b[32m    83.5234\u001b[0m | \u001b[32m 241.9195\u001b[0m | \u001b[32m 123.6935\u001b[0m | \u001b[32m  68.3404\u001b[0m | \n",
      "    2 | 00m56s |   -0.91318 |    0.0097 |     70.8652 |  268.5220 |  127.4504 |   74.4065 | \n",
      "    3 | 00m59s |   -0.91554 |    0.0038 |     77.9345 |  220.4452 |  133.8224 |   60.0023 | \n",
      "    4 | 00m47s | \u001b[35m  -0.91168\u001b[0m | \u001b[32m   0.0072\u001b[0m | \u001b[32m    57.0193\u001b[0m | \u001b[32m 287.8117\u001b[0m | \u001b[32m 135.8707\u001b[0m | \u001b[32m  66.0467\u001b[0m | \n",
      "    5 | 00m44s |   -0.91220 |    0.0089 |     59.9051 |  202.7388 |  141.5527 |   62.9351 | \n",
      "\u001b[31mBayesian Optimization\u001b[0m\n",
      "\u001b[94m-------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |        lr |   num_epoch |     unit1 |     unit2 |     unit3 | \n",
      "    6 | 00m47s |   -0.91586 |    0.0074 |     50.5080 |  220.3130 |  120.0727 |   60.5185 | \n",
      "    7 | 01m48s |   -0.91177 |    0.0064 |     99.2336 |  299.3485 |  159.5697 |   78.9930 | \n",
      "    8 | 01m46s |   -0.91649 |    0.0022 |     98.6086 |  201.4427 |  120.2826 |   78.3397 | \n",
      "    9 | 01m17s |   -0.91930 |    0.0068 |     95.4930 |  299.1188 |  120.5902 |   60.0451 | \n",
      "   10 | 00m50s |   -0.91402 |    0.0089 |     50.9448 |  242.7989 |  159.6339 |   78.8817 | \n",
      "Iteration 0: \n",
      "\tmax\n",
      "Iteration 1: \n",
      "\tall\n",
      "{'unit3': 66.0466514526368, 'unit2': 135.8706989692268, 'unit1': 287.8117436390945, 'num_epoch': 57.01934692976169, 'lr': 0.007230903541023828}\n"
     ]
    }
   ],
   "source": [
    "def get_model(unit1, unit2, unit3):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(int(unit1), input_dim = 58))\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(int(unit2)))\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(int(unit3)))\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('linear'))\n",
    "    return model\n",
    "\n",
    "def fit_with(unit1, unit2, unit3, lr, num_epoch):\n",
    "    model = get_model(unit1, unit2, unit3)\n",
    "    sgd = SGD(lr=lr, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss = 'mean_squared_error', optimizer = sgd)\n",
    "    model.fit(x_ptrain, y_ptrain, epochs = int(num_epoch), batch_size = 200, verbose = 0)\n",
    "    pred = model.predict(x_ptest)\n",
    "    rmse = np.sqrt(mean_squared_error(pred, y_ptest))\n",
    "    return -rmse\n",
    "\n",
    "pbounds = {'unit1': (200, 300), 'unit2': (120, 160), 'unit3': (60, 80),\n",
    "           'lr': (1e-3, 1e-2), 'num_epoch': (50, 100)}\n",
    "\n",
    "optimizer = BayesianOptimization(\n",
    "    f = fit_with,\n",
    "    pbounds = pbounds,\n",
    "    verbose=2,  # verbose = 1 prints only when a maximum is observed, verbose = 0 is silent\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "optimizer.maximize(init_points=5, n_iter=5,)\n",
    "\n",
    "\n",
    "for i, res in enumerate(optimizer.res):\n",
    "    print(\"Iteration {}: \\n\\t{}\".format(i, res))\n",
    "\n",
    "print(optimizer.res['max']['max_params'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.9372867159486481\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(int(optimizer.res['max']['max_params']['unit1']), input_dim = 58))\n",
    "model.add(Activation('tanh'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(int(optimizer.res['max']['max_params']['unit2'])))\n",
    "model.add(Activation('tanh'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(int(optimizer.res['max']['max_params']['unit3'])))\n",
    "model.add(Activation('tanh'))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('linear'))\n",
    "\n",
    "sgd = SGD(lr=optimizer.res['max']['max_params']['lr'], decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss = 'mean_squared_error', optimizer = sgd)\n",
    "model.fit(x_train, y_train, epochs = int(optimizer.res['max']['max_params']['num_epoch']), batch_size = 200, verbose = 0)\n",
    "pred = model.predict(x_test)\n",
    "rmse = np.sqrt(mean_squared_error(pred, y_test))\n",
    "print \"RMSE:\", rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
